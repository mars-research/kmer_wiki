# 3 Oct 2019

Created: Oct 11, 2019 2:13 AM
Created By: Harishankar Vishwanathan
Last Edited Time: Nov 17, 2019 11:46 PM

Atomic operations are not that expensive, bouncing the cache lines is expensive

### Discussion about Pipeline Parallelism

Consider a data item on which we need to do an operation F(). Requests come in the form of a stream of data. Consider a 3 core machine. 

Request parallelism:

If each request is independent of each other, split the data item into say 3 chunks. Each n/3 large chunk is operated upon by one core. Each core runs F() on each data item in the chunk in parallel.

Problem: 

- if F needs a data structure, the cache lines related to the data structure will be present in L1, L2 (and a shared L3) across 3 different cores as f1 runs in all the 3 cores. The cache hierarchy is polluted by this data structure (implications: to bring something else in the cache would require eviction of the said cache line)
- Writes to the data structure would be clearly bad as they would require acquisition of locks and the cache lines related to the data structure to bounce between cores.

Pipeline parallelism:

- Since each request is independent of each other, split the operation F into pipelines f1()→f2()→f3().
- f1() runs only on core1, f2() on core 2, and so on.
- Data structures needed by f1 are cached only in core 1's caches.

TODO

- Read and get idea of what's happening in bqueues code
- What is prefetch instruction in x86

Property